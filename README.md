---
language: ko
tags:
- kogpt2
- gpt2
- text-generation
license: MIT
datasets:
- custom
metrics:
- none
---
**This model is no longer actively in training due to lack of computing resources. NIPAì¸¡ì˜ ì •ì±… ë³€ê²½ìœ¼ë¡œ ê³ ì„±ëŠ¥ì»´í“¨íŒ… ì§€ì›ëŒ€ìƒì—ì„œ ì œì™¸ë¨ì— ë”°ë¼, ëª¨ë¸ í›ˆë ¨ì„ ë” ì´ìƒ ì§„í–‰í•  ìˆ˜ ì—†ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.**

# Model name
KoGPT2

<a href="https://www.buymeacoffee.com/ksjae" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/default-orange.png" alt="Buy Me A Coffee" height="41" width="174"></a>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1s5zZZL8j2waMTkwUOmSOv6IywoBrNm1z?usp=sharing)

Demo **NOT** available at : [ì•„ë¬´ë§ ëŒ€ì”ì¹˜](https://text.ksjit.com)

## Model description
GPT2 and GPT3 trained on ~40GB of Korean datasets.
see the included json files for hyperparameter details.

Available models (Training ATM):
- KoGPT2-base(117M)
- KoGPT2-medium(345M)
- KoGPT2-large(774M)
- KoGPT2-xlarge(1.5B)
- KoGPT2-2.7B *TBA*

Models are available as TF checkpoint files [Training script](https://github.com/ksjae/KoGPT2-train) or [Huggingface transformers](https://github.com/huggingface/transformers.git) compatible ones

n_ctx available : 1024 2048 384

## Intended uses & limitations
Intended for **Korean** text generation for ai-text-adventure(https://github.com/ksjae/ai-text-adventure) with PPLM.

### Downloads
Download files from links at the Releases tab.
Alternatively, it is available from [my own server(wget-friendly)](https://static.ksjit.com)

#### How to use

Try out on [colab](https://colab.research.google.com/drive/1s5zZZL8j2waMTkwUOmSOv6IywoBrNm1z?usp=sharing)

or go to [KoGPT2-train](https://github.com/ksjae/KoGPT2-train) and use scripts/demo.py 

#### Limitations and bias

v0.1 may have faulty tokenizers, producing bad outputs.

v0.2+ be GPT2 with n_ctx of 2048. True form of GPT-3 implementation(alternating layers) will not be available within the year.

If other limitations or errors are found, please open an issue.

## Training data

Initialized with GPT(774M,https://github.com/openai/gpt-2/blob/master/model_card.md).

The following data was used, and is available for redistribution [here](https://static.ksjit.com/datasets):

- Namuwiki database dump, Early 2020
- KCC(Kookmin University Corpus)
- Dump of Korean Wikipedia
- NAVER movie reviews
- Korean news(about 1GB) from Leipzig(a German university)
- Context data from KorSQUAD questions
- Parsed Korean CommonCrawl data(WIP)

Please note the completed dataset includes <|endoftext|> tags.

The following data were used, but is unavailable for redistribution:

- Sejong Corpus
- 'ëª¨ë‘ì˜ ë§ë­‰ì¹˜' from corpus.korean.go.kr
- A *PRIVATE* collection of korean novels
- Webcrawl of modern, uploaded text novels('í…ë³¸' - If you want to prevent your novel from going in the training set, please contact me and I will blacklist it)
- Game storylines (with authors' approval)


## Training procedure

All hyperparameters are the same as GPT2-large
One paragraph per line(TextDataset)

Early models(GPT2-large v0.2 and prior) are trained on 2xTesla V100 for 3~4 weeks.
Models up to XL size are trained on v3-8 TPUs.

## Eval results
> prompt >>> ë‚˜ëŠ” ì–´ë‘ìš´ ìˆ² ì†ì„ ê±°ë‹ê³  ìˆë‹¤.

### v0.1 (xl)
> ì–´ëŠìƒˆ ë‚˜ëŠ” ê·¸ ì‘ì€ ì‚°ë“¤ì„ í–¥í•´ ë›°ê¸° ì‹œì‘í–ˆë‹¤. ê·¸ë¦¬ê³  ë‚´ ì‹œì•¼ì—ëŠ” ì´ ê³¨ì§œê¸°ì— ëŒ€í•œ ì–´ë–¤ ë™ê²½ë„, í˜¹ì€ ë™ê²½ê³¼ í™˜í¬ì¡°ì°¨ ì—¿ë³´ì˜€ë‹¤ê°€ ì‚¬ë¼ì¡Œë‹¤ê°€ëŠ” ì‚¬ë¼ì ¸ ë²„ë¦¬ê³  ë§ì•˜ ë‹¤. ê·¸ëŸ¬ë‚˜ ë‚˜ ì—­ì‹œ ê·¸ê²ƒì„ ë¯¿ì§€ ì•Šì•˜ë‹¤. ì•„ë‹ˆ ê·¸ê²ƒë„ ëª¨ë¥¸ë‹¤â€¦â€¦. ê·¸ë ‡ë‹¤ë©´ ê·¸ê²ƒì€ ë˜ ë¬´ìŠ¨ ë§ì¸ê°€? ë‚´ê°€ ì´ë ‡ê²Œ ë§í•´ë„ ì¢‹ì„ í…ë°â€¦â€¦ í•˜ì§€ë§Œ ì–´ì¨Œë“  ì´ê³³ì—” ì™œ ê·¸ë¦¬ ë¨¼ ê³³ì—ì„œë¶€í„° ì°¾ì•„ì˜¬ê¹Œ í•˜ê³  ìƒê°í•´ ë³´ì•˜ë‹¤. ê·¸ë˜ì„œ ê·¸ëŠ” ì§€ê¸ˆ ì–´ëŠ í•œ ê³³ë§Œì„ í—¤ë§¤ë©´ì„œ ì ê¾¸ë§Œ ê±¸ì–´ì˜¤ëŠ” ê²ƒì¼ê¹Œ?

### v0.2-WEB (xl)
> ë‚˜ëŠ” ì–´ë‘ìš´ ìˆ² ì†ì„ ê±°ë‹ê³  ìˆë‹¤. #ì•¨ë¦¬ê±´íŠ¸í…Œì´ë¸” ì˜ #ì½”ì½”ë„›ì ¤ë¦¬ ìƒ‰ì´ ë„˜ ì˜ˆì˜ê³  ë§›ë‚˜ë‹¤. #ê·¸ë¦­ìš”ê±°íŠ¸ ì›ë¬¼ì´ ë“¤ì–´ê°„ ë§ˆì§€ë§‰  ë² ì´ì»¤ë¦¬ #ë§ì°¨ì´ˆì½”ì¼€ì´í¬ ë„ ë§›ë‚˜ê³  ê²‰ì— ì´ˆì½”ë„ ë‘ê»ê²Œ ì”¹íˆê³ ..âœ¨ğŸ’•ğŸ’•ğŸ‘ğŸ» . . #ì•¨ë¦¬ê±´íŠ¸í…Œì´ë¸” @eleganttable_  #ê°„ì‹ #ì¹´í˜ì–´ë‹ˆì–¸ë² ì´ì»¤ë¦¬ë² ì´ì»¤ #ì½©ì½©ë³¼ #ì¿ í‚¤ #ë””ì €íŠ¸ë§›ì§‘ #ë””ì €íŠ¸ì¹´í˜ #dessert #ì¹´í˜ìŠ¤íƒ€ê·¸ë¨ #ë§›ìŠ¤íƒ€ê·¸ë¨ #ë¨¹ìŠ¤íƒ€ ê·¸ë¨ #ë¨¹ë°© #ì¼ìƒ #daily

### v0.2-story (xl)
> ë‚˜ëŠ” ì–´ë‘ìš´ ìˆ² ì†ì„ ê±°ë‹ê³  ìˆë‹¤. ì •ëª…ì´ ì“´ì›ƒìŒì„ ì§€ì€ ì±„ ê³³ì— ê±¸ì–´ë‚˜ì™”ë‹¤. ... ... ... ... ì‚¬ë§íƒ‘ ë†’ì´ê¹Œì§€ ì˜¬ë¼ê°€ì•¼í•˜ë‚˜?


### BibTeX entry and citation info

```bibtex
@unpublished{CitekeyUnpublished,
      author = "Seungjae Kim",
      title  = "Introducing larger KoGPT2",
      year   = 2020
    }
```
